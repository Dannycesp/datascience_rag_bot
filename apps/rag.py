import json
import re

from time import time

from openai import OpenAI

import ingest

import os

from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")

#api_key=os.environ['OPENAI_API_KEY']


client = OpenAI(api_key=api_key)
index = ingest.load_index()


def search(query):
    boost = {
        'Question': 0.56,
        'Answer': 1.46}
    
    results = index.search(
        query=query, filter_dict={}, boost_dict=boost, num_results=10
    )

    return results


prompt_template = """
Your role is to answer questions about datascience based on the database with questions and asnwers about datascience.
Use only the facts from the CONTEXT when answering the QUESTION, where the CONTEXT is just our question and answer database, that you have below.
If the context is not sufficient for giving a response, your response will be "Not enough context in questions and answers database".
Even if you have more information outside the context, do not use it and base your answer only in the context.
Write only the answer itself without any further comments.
Be precise and concise and try using as few words as required to give a meaninful answer, with the minimum words possible.

QUESTION: {question}

CONTEXT:
{context}
""".strip()

entry_template = """
Question: {Question}
Answer: {Answer}

""".strip()

def build_prompt(query, search_results):
    context = ""
    
    for doc in search_results:
        context = context + entry_template.format(**doc) + "\n\n"

    prompt = prompt_template.format(question=query, context=context).strip()
    return prompt
    

def llm(prompt, model="gpt-4o-mini"):
    response = client.chat.completions.create(
        model=model, messages=[{"role": "user", "content": prompt}]
    )

    answer = response.choices[0].message.content

    token_stats = {
        "prompt_tokens": response.usage.prompt_tokens,
        "completion_tokens": response.usage.completion_tokens,
        "total_tokens": response.usage.total_tokens,
    }

    return answer, token_stats


evaluation_prompt_template = """
You have to evaluate a Retrieval Augmented Generation System (RAG).
Your task consists in analyzing the relevance of the answer generated by a large language model (llm) to a given question.
Based on the relevance of the generated answer, you will classify it
as "NON_RELEVANT", "PARTLY_RELEVANT", or "RELEVANT".

Here is the data for evaluation:

Question: {question}
Generated Answer: {answer_llm}

Please analyze the content and context of the generated answer in relation to the question
and provide your evaluation in parsable JSON without using code blocks with this structure:

{{
  "Relevance": "NON_RELEVANT" | "PARTLY_RELEVANT" | "RELEVANT",
  "Explanation": "[Provide a brief explanation for your evaluation]"
}}

 The final answer must be just the json object without any further comments before or after.

""".strip()

#FUNCTION TO PARSE THE OUTPUTS FROM THE LLM WHEN IT DOES NOT PRODUCE PURE JSON CONSISTENTLY (usually open source models)
#after many iterations this is the function that works better when the output of the llm is not correctly formatted
#and we have more control of which outputs failed as we save the errors and the program continue even if some
#outputs failed



def robust_json_loads(s):
    """
    Attempts to parse a JSON string, fixing common errors if parsing fails.

    Parameters:
    s (str): The JSON string to parse.

    Returns:
    tuple: (success (bool), data (dict or None), error_message (str or None))
    """
    original_s = s  # Keep a copy of the original string

    # First, attempt to parse the input string directly
    try:
        data = json.loads(s)
        return (True, data, None)
    except json.JSONDecodeError:
        pass  # Proceed to cleaning steps if parsing fails

    # Step 1: Strip wrapping backticks and language specifiers
    s = s.strip()
    s = re.sub(r'^```[a-zA-Z]*\s*', '', s)  # Remove starting triple backticks and optional language
    s = re.sub(r'```$', '', s)              # Remove ending triple backticks
    s = s.strip('`')                        # Remove any remaining backticks

    # Step 2: Remove any text before the first '{' or '['
    start_idx = re.search(r'[\{\[]', s)
    if not start_idx:
        error_message = "No JSON object could be detected in the input."
        return (False, None, error_message)
    s = s[start_idx.start():]

    # Step 3: Remove any text after the last '}' or ']'
    end_idx = max(s.rfind('}'), s.rfind(']'))
    if end_idx == -1:
        error_message = "No JSON object could be detected in the input."
        return (False, None, error_message)
    s = s[:end_idx+1]

    # Step 4: Remove extraneous characters after the JSON content
    # Remove any characters after the last closing brace/bracket
    s = re.sub(r'([\}\]])[\s\S]*$', r'\1', s)

    # Step 5: Remove extraneous characters before the JSON content
    # Remove any characters before the first opening brace/bracket
    s = re.sub(r'^[\s\S]*?([\{\[])', r'\1', s)

    # Step 6: Replace single quotes with double quotes for keys and values
    # Avoid changing single quotes inside double-quoted strings
    s = re.sub(
        r'(?<=[:\{\[,])\s*\'([^\']*)\'\s*(?=[:,\}\]])',
        r'"\1"',
        s
    )

    # Step 7: Remove trailing commas before closing braces/brackets
    s = re.sub(r',\s*(\}|\])', r'\1', s)

    # Step 8: Balance brackets and braces if necessary
    def balance_characters(s, open_char, close_char):
        opens = s.count(open_char)
        closes = s.count(close_char)
        if opens > closes:
            s += close_char * (opens - closes)
        elif closes > opens:
            s = open_char * (closes - opens) + s
        return s

    s = balance_characters(s, '{', '}')
    s = balance_characters(s, '[', ']')

    # Step 9: Remove unescaped control characters
    # Control characters are not allowed in JSON strings
    s = re.sub(r'[\x00-\x1F]+', '', s)

    # Step 10: Final check to remove extra double quotes at the end of strings in arrays
    # This targets the specific case you mentioned
    s = re.sub(r'(".*?")"+(?=\s*[\],}])', r'\1', s)

    # Step 11: Attempt to parse the cleaned string
    try:
        data = json.loads(s)
    except json.JSONDecodeError as e:
        error_message = f"Error parsing JSON after cleaning: {e}"
        return (False, None, error_message)

    # Validate that the parsed data contains the expected 'questions' key
    if not isinstance(data, dict) or 'questions' not in data:
        error_message = "Parsed JSON does not contain 'questions' key."
        return (False, None, error_message)

    # Check that 'questions' is a non-empty list
    if not isinstance(data['questions'], list) or not data['questions']:
        error_message = "'questions' key is empty or not a list."
        return (False, None, error_message)

    # Parsing and validation successful
    return (True, data, None)




def evaluate_relevance(question, answer):
    prompt = evaluation_prompt_template.format(question=question, answer_llm=answer)
    evaluation, tokens = llm(prompt, model="gpt-4o-mini")
    
    #call to the add hoc parsing function which returns json in evaluation
    #success is true/false and error_message the error raised by json_loads
    #or other errors contemplated...see above
    
    success, evaluation, error_message = robust_json_loads(evaluation)

    return evaluation, tokens


def calculate_openai_cost(model, tokens):
    openai_cost = 0

    if model == "gpt-4o-mini":
        openai_cost = (
            tokens["prompt_tokens"] * 0.00015 + tokens["completion_tokens"] * 0.0006
        ) / 1000
    else:
        print("Model not recognized. OpenAI cost calculation failed.")

    return openai_cost


def rag(query, model="gpt-4o-mini"):
    t0 = time()

    search_results = search(query)
    prompt = build_prompt(query, search_results)
    answer, token_stats = llm(prompt, model=model)

    relevance, rel_token_stats = evaluate_relevance(query, answer)

    t1 = time()
    took = t1 - t0

    openai_cost_rag = calculate_openai_cost(model, token_stats)
    openai_cost_eval = calculate_openai_cost(model, rel_token_stats)

    openai_cost = openai_cost_rag + openai_cost_eval

    answer_data = {
        "answer": answer,
        "model_used": model,
        "response_time": took,
        "relevance": relevance.get("Relevance", "UNKNOWN"),
        "relevance_explanation": relevance.get(
            "Explanation", "Failed to parse evaluation"
        ),
        "prompt_tokens": token_stats["prompt_tokens"],
        "completion_tokens": token_stats["completion_tokens"],
        "total_tokens": token_stats["total_tokens"],
        "eval_prompt_tokens": rel_token_stats["prompt_tokens"],
        "eval_completion_tokens": rel_token_stats["completion_tokens"],
        "eval_total_tokens": rel_token_stats["total_tokens"],
        "openai_cost": openai_cost,
    }

    return answer_data
